{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "params = {'font.size':16,\n",
    "          'legend.fontsize': 'large',\n",
    "          'figure.figsize': (11.69, 8.27),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'large',\n",
    "          'xtick.labelsize':'large',\n",
    "          'ytick.labelsize':'large'}\n",
    "plt.rcParams.update(params)\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import math\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a88561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the memory test related data from my csv files and in the following steps cleaning the data and re-arranging them into dictionaries\n",
    "\n",
    "file_name = 'perf{}.csv'\n",
    "d_1={}\n",
    "d_2={}\n",
    "for i in range(1, 29):\n",
    "    d_1['df{0}'.format(i)] = pd.read_csv(file_name.format(i))\n",
    "    d_2['df{0}'.format(i)] = pd.read_csv(file_name.format(i))\n",
    "for i in range(29, 51):\n",
    "    d_1['df{0}'.format(i)] = pd.read_csv(file_name.format(i))\n",
    "    d_2['df{0}'.format(i)] = pd.read_csv(file_name.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 29):\n",
    "    d_1['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Load'] == 1, ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1['df{0}'.format(i)]=d_1['df{0}'.format(i)].dropna()\n",
    "    d_2['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Load'] == 2, ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2['df{0}'.format(i)]=d_2['df{0}'.format(i)].dropna()\n",
    "for i in range(29, 51):\n",
    "    d_1['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Load'] == 1, ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1['df{0}'.format(i)]=d_1['df{0}'.format(i)].dropna()\n",
    "    d_2['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Load'] == 2, ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2['df{0}'.format(i)]=d_2['df{0}'.format(i)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1_3={}\n",
    "d_1_4={}\n",
    "d_1_5={}\n",
    "d_1_6={}\n",
    "d_2_3={}\n",
    "d_2_4={}\n",
    "d_2_5={}\n",
    "d_2_6={}\n",
    "for i in range(1, 29):\n",
    "    d_1_3['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 3, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_4['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 4, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_5['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 5, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_6['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 6, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_3['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 3, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_4['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 4, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_5['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 5, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_6['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 6, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "for i in range(29, 51):\n",
    "    d_1_3['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 3, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_4['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 4, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_5['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 5, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_6['df{0}'.format(i)]=d_1['df{0}'.format(i)].loc[d_1['df{0}'.format(i)]['Distractor'] == 6, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_3['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 3, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_4['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 4, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_5['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 5, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_6['df{0}'.format(i)]=d_2['df{0}'.format(i)].loc[d_2['df{0}'.format(i)]['Distractor'] == 6, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1_3_1 ={}\n",
    "d_1_4_1 ={}\n",
    "d_1_5_1 ={}\n",
    "d_1_6_1 ={}\n",
    "d_2_3_1 ={}\n",
    "d_2_4_1 ={}\n",
    "d_2_5_1 ={}\n",
    "d_2_6_1 ={}\n",
    "for i in range(1, 29):\n",
    "    d_1_3_1['df{0}'.format(i)]=d_1_3['df{0}'.format(i)].loc[d_1_3['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_4_1['df{0}'.format(i)]=d_1_4['df{0}'.format(i)].loc[d_1_4['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_5_1['df{0}'.format(i)]=d_1_5['df{0}'.format(i)].loc[d_1_5['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_6_1['df{0}'.format(i)]=d_1_6['df{0}'.format(i)].loc[d_1_6['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_3_1['df{0}'.format(i)]=d_2_3['df{0}'.format(i)].loc[d_2_3['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_4_1['df{0}'.format(i)]=d_2_4['df{0}'.format(i)].loc[d_2_4['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_5_1['df{0}'.format(i)]=d_2_5['df{0}'.format(i)].loc[d_2_5['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_6_1['df{0}'.format(i)]=d_2_6['df{0}'.format(i)].loc[d_2_6['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "for i in range(29, 51):\n",
    "    d_1_3_1['df{0}'.format(i)]=d_1_3['df{0}'.format(i)].loc[d_1_3['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_4_1['df{0}'.format(i)]=d_1_4['df{0}'.format(i)].loc[d_1_4['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_5_1['df{0}'.format(i)]=d_1_5['df{0}'.format(i)].loc[d_1_5['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_1_6_1['df{0}'.format(i)]=d_1_6['df{0}'.format(i)].loc[d_1_6['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_3_1['df{0}'.format(i)]=d_2_3['df{0}'.format(i)].loc[d_2_3['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_4_1['df{0}'.format(i)]=d_2_4['df{0}'.format(i)].loc[d_2_4['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_5_1['df{0}'.format(i)]=d_2_5['df{0}'.format(i)].loc[d_2_5['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]\n",
    "    d_2_6_1['df{0}'.format(i)]=d_2_6['df{0}'.format(i)].loc[d_2_6['df{0}'.format(i)]['Perform'] == 1, \\\n",
    "                                                        ['Load','Distractor','Perform','Rtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "list131rt = []\n",
    "list141rt = []\n",
    "list151rt = []\n",
    "list161rt = []\n",
    "list231rt = []\n",
    "list241rt = []\n",
    "list251rt = []\n",
    "list261rt = []\n",
    "for i in range(1, 29):\n",
    "    list131rt.append(d_1_3_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list141rt.append(d_1_4_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list151rt.append(d_1_5_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list161rt.append(d_1_6_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list231rt.append(d_2_3_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list241rt.append(d_2_4_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list251rt.append(d_2_5_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list261rt.append(d_2_6_1['df{0}'.format(i)].Rtime.mean())\n",
    "for i in range(29, 51):\n",
    "    list131rt.append(d_1_3_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list141rt.append(d_1_4_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list151rt.append(d_1_5_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list161rt.append(d_1_6_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list231rt.append(d_2_3_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list241rt.append(d_2_4_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list251rt.append(d_2_5_1['df{0}'.format(i)].Rtime.mean())\n",
    "    list261rt.append(d_2_6_1['df{0}'.format(i)].Rtime.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "list131rtstd = []\n",
    "list141rtstd = []\n",
    "list151rtstd = []\n",
    "list161rtstd = []\n",
    "list231rtstd = []\n",
    "list241rtstd = []\n",
    "list251rtstd = []\n",
    "list261rtstd = []\n",
    "for i in range(1, 29):\n",
    "    list131rtstd.append(d_1_3_1['df{0}'.format(i)].Rtime.std())\n",
    "    list141rtstd.append(d_1_4_1['df{0}'.format(i)].Rtime.std())\n",
    "    list151rtstd.append(d_1_5_1['df{0}'.format(i)].Rtime.std())\n",
    "    list161rtstd.append(d_1_6_1['df{0}'.format(i)].Rtime.std())\n",
    "    list231rtstd.append(d_2_3_1['df{0}'.format(i)].Rtime.std())\n",
    "    list241rtstd.append(d_2_4_1['df{0}'.format(i)].Rtime.std())\n",
    "    list251rtstd.append(d_2_5_1['df{0}'.format(i)].Rtime.std())\n",
    "    list261rtstd.append(d_2_6_1['df{0}'.format(i)].Rtime.std())\n",
    "for i in range(29, 51):\n",
    "    list131rtstd.append(d_1_3_1['df{0}'.format(i)].Rtime.std())\n",
    "    list141rtstd.append(d_1_4_1['df{0}'.format(i)].Rtime.std())\n",
    "    list151rtstd.append(d_1_5_1['df{0}'.format(i)].Rtime.std())\n",
    "    list161rtstd.append(d_1_6_1['df{0}'.format(i)].Rtime.std())\n",
    "    list231rtstd.append(d_2_3_1['df{0}'.format(i)].Rtime.std())\n",
    "    list241rtstd.append(d_2_4_1['df{0}'.format(i)].Rtime.std())\n",
    "    list251rtstd.append(d_2_5_1['df{0}'.format(i)].Rtime.std())\n",
    "    list261rtstd.append(d_2_6_1['df{0}'.format(i)].Rtime.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list13rt = []\n",
    "list14rt = []\n",
    "list15rt = []\n",
    "list16rt = []\n",
    "list23rt = []\n",
    "list24rt = []\n",
    "list25rt = []\n",
    "list26rt = []\n",
    "for i in range(1, 29):\n",
    "    list13rt.append(d_1_3['df{0}'.format(i)].Rtime.mean())\n",
    "    list14rt.append(d_1_4['df{0}'.format(i)].Rtime.mean())\n",
    "    list15rt.append(d_1_5['df{0}'.format(i)].Rtime.mean())\n",
    "    list16rt.append(d_1_6['df{0}'.format(i)].Rtime.mean())\n",
    "    list23rt.append(d_2_3['df{0}'.format(i)].Rtime.mean())\n",
    "    list24rt.append(d_2_4['df{0}'.format(i)].Rtime.mean())\n",
    "    list25rt.append(d_2_5['df{0}'.format(i)].Rtime.mean())\n",
    "    list26rt.append(d_2_6['df{0}'.format(i)].Rtime.mean())\n",
    "for i in range(29, 51):\n",
    "    list13rt.append(d_1_3['df{0}'.format(i)].Rtime.mean())\n",
    "    list14rt.append(d_1_4['df{0}'.format(i)].Rtime.mean())\n",
    "    list15rt.append(d_1_5['df{0}'.format(i)].Rtime.mean())\n",
    "    list16rt.append(d_1_6['df{0}'.format(i)].Rtime.mean())\n",
    "    list23rt.append(d_2_3['df{0}'.format(i)].Rtime.mean())\n",
    "    list24rt.append(d_2_4['df{0}'.format(i)].Rtime.mean())\n",
    "    list25rt.append(d_2_5['df{0}'.format(i)].Rtime.mean())\n",
    "    list26rt.append(d_2_6['df{0}'.format(i)].Rtime.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list13rtstd = []\n",
    "list14rtstd = []\n",
    "list15rtstd = []\n",
    "list16rtstd = []\n",
    "list23rtstd = []\n",
    "list24rtstd = []\n",
    "list25rtstd = []\n",
    "list26rtstd = []\n",
    "for i in range(1, 29):\n",
    "    list13rtstd.append(d_1_3['df{0}'.format(i)].Rtime.std())\n",
    "    list14rtstd.append(d_1_4['df{0}'.format(i)].Rtime.std())\n",
    "    list15rtstd.append(d_1_5['df{0}'.format(i)].Rtime.std())\n",
    "    list16rtstd.append(d_1_6['df{0}'.format(i)].Rtime.std())\n",
    "    list23rtstd.append(d_2_3['df{0}'.format(i)].Rtime.std())\n",
    "    list24rtstd.append(d_2_4['df{0}'.format(i)].Rtime.std())\n",
    "    list25rtstd.append(d_2_5['df{0}'.format(i)].Rtime.std())\n",
    "    list26rtstd.append(d_2_6['df{0}'.format(i)].Rtime.std())\n",
    "for i in range(29, 51):\n",
    "    list13rtstd.append(d_1_3['df{0}'.format(i)].Rtime.std())\n",
    "    list14rtstd.append(d_1_4['df{0}'.format(i)].Rtime.std())\n",
    "    list15rtstd.append(d_1_5['df{0}'.format(i)].Rtime.std())\n",
    "    list16rtstd.append(d_1_6['df{0}'.format(i)].Rtime.std())\n",
    "    list23rtstd.append(d_2_3['df{0}'.format(i)].Rtime.std())\n",
    "    list24rtstd.append(d_2_4['df{0}'.format(i)].Rtime.std())\n",
    "    list25rtstd.append(d_2_5['df{0}'.format(i)].Rtime.std())\n",
    "    list26rtstd.append(d_2_6['df{0}'.format(i)].Rtime.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "list13 = []\n",
    "list14 = []\n",
    "list15 = []\n",
    "list16 = []\n",
    "list23 = []\n",
    "list24 = []\n",
    "list25 = []\n",
    "list26 = []\n",
    "for i in range(1, 29):\n",
    "    list1.append(d_1['df{0}'.format(i)].Perform.mean())\n",
    "    list2.append(d_2['df{0}'.format(i)].Perform.mean())\n",
    "    list13.append(d_1_3['df{0}'.format(i)].Perform.mean())\n",
    "    list14.append(d_1_4['df{0}'.format(i)].Perform.mean())\n",
    "    list15.append(d_1_5['df{0}'.format(i)].Perform.mean())\n",
    "    list16.append(d_1_6['df{0}'.format(i)].Perform.mean())\n",
    "    list23.append(d_2_3['df{0}'.format(i)].Perform.mean())\n",
    "    list24.append(d_2_4['df{0}'.format(i)].Perform.mean())\n",
    "    list25.append(d_2_5['df{0}'.format(i)].Perform.mean())\n",
    "    list26.append(d_2_6['df{0}'.format(i)].Perform.mean())\n",
    "for i in range(29, 51):\n",
    "    list1.append(d_1['df{0}'.format(i)].Perform.mean())\n",
    "    list2.append(d_2['df{0}'.format(i)].Perform.mean())\n",
    "    list13.append(d_1_3['df{0}'.format(i)].Perform.mean())\n",
    "    list14.append(d_1_4['df{0}'.format(i)].Perform.mean())\n",
    "    list15.append(d_1_5['df{0}'.format(i)].Perform.mean())\n",
    "    list16.append(d_1_6['df{0}'.format(i)].Perform.mean())\n",
    "    list23.append(d_2_3['df{0}'.format(i)].Perform.mean())\n",
    "    list24.append(d_2_4['df{0}'.format(i)].Perform.mean())\n",
    "    list25.append(d_2_5['df{0}'.format(i)].Perform.mean())\n",
    "    list26.append(d_2_6['df{0}'.format(i)].Perform.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(list1,list2,list13,list14,list15,list16,list23,list24,list25,list26,\\\n",
    "    list13rt,list14rt,list15rt,list16rt,list23rt,list24rt,list25rt,list26rt,\\\n",
    "    list13rtstd,list14rtstd,list15rtstd,list16rtstd,list23rtstd,list24rtstd,list25rtstd,list26rt,\\\n",
    "    list131rt, list141rt, list151rt, list161rt, list231rt, list241rt, list251rt, list261rt,\\\n",
    "    list131rtstd, list141rtstd, list151rtstd, list161rtstd, list231rtstd, list241rtstd, list251rtstd, list261rtstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c086af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the dataframe with all my data\n",
    "\n",
    "Pload_df = pd.DataFrame(zip(list1,list2,\\\n",
    "                            list13,list14,list15,list16,\\\n",
    "                            list23,list24,list25,list26,\\\n",
    "                            list13rt,list14rt,list15rt,list16rt,\\\n",
    "                            list23rt,list24rt,list25rt,list26rt,\\\n",
    "                            list13rtstd,list14rtstd,list15rtstd,list16rtstd,\\\n",
    "                            list23rtstd,list24rtstd,list25rtstd,list26rtstd,\\\n",
    "                            list131rt, list141rt, list151rt, list161rt,\\\n",
    "                            list231rt, list241rt, list251rt, list261rt,\\\n",
    "                            list131rtstd, list141rtstd, list151rtstd, list161rtstd,\\\n",
    "                            list231rtstd, list241rtstd, list251rtstd, list261rtstd),\\\n",
    "                        columns = ['Perf_Load_1', 'Perf_Load_2',\\\n",
    "                                   'Perf_Load_1_3', 'Perf_Load_1_4','Perf_Load_1_5', 'Perf_Load_1_6',\\\n",
    "                                   'Perf_Load_2_3', 'Perf_Load_2_4','Perf_Load_2_5', 'Perf_Load_2_6',\\\n",
    "                                   'Rtime_1_3', 'Rtime_1_4', 'Rtime_1_5','Rtime_1_6',\\\n",
    "                                   'Rtime_2_3', 'Rtime_2_4', 'Rtime_2_5', 'Rtime_2_6',\\\n",
    "                                   'Rtime_1_3std', 'Rtime_1_4std', 'Rtime_1_5std','Rtime_1_6std',\\\n",
    "                                   'Rtime_2_3std', 'Rtime_2_4std', 'Rtime_2_5std', 'Rtime_2_6std',\\\n",
    "                                   'Rtime_1_3_1', 'Rtime_1_4_1', 'Rtime_1_5_1', 'Rtime_1_6_1',\\\n",
    "                                   'Rtime_2_3_1', 'Rtime_2_4_1', 'Rtime_2_5_1', 'Rtime_2_6_1',\\\n",
    "                                   'Rtime_1_3_1std', 'Rtime_1_4_1std', 'Rtime_1_5_1std','Rtime_1_6_1std',\\\n",
    "                                   'Rtime_2_3_1std', 'Rtime_2_4_1std', 'Rtime_2_5_1std', 'Rtime_2_6_1std'])\n",
    "Pload_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac172c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading all the remaining data for WISC values, and the average performance values that I generated from the original dataset\n",
    "\n",
    "Paverage_df = pd.read_csv('Performance2.csv')\n",
    "Paverage_df =Paverage_df.loc[Paverage_df['Group'] != \"on-ADHD\", ['Group','Age','IQscore','Performance', 'Digit_span'\\\n",
    "                                                                 , 'Picture_completion', 'Processing_speed',\\\n",
    "                                                                'FFD', 'Arithmetic', 'FD','VT','KRatio','CumK','Pupil_diameter_std','SacFreq','FixDur',\\\n",
    "                                                                 'exp','exp1','exp2','exp3','exp4','exp5','exp6']]\n",
    "Paverage_df = Paverage_df.replace(to_replace=\"off-ADHD\",\n",
    "           value=1);\n",
    "Paverage_df = Paverage_df.replace(to_replace=\"Ctrl\",\n",
    "           value=0);\n",
    "Paverage_df.index = range(0,50)\n",
    "Paverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b61ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes and review the result\n",
    "Performance_df = pd.concat([Paverage_df,Pload_df], axis=1)\n",
    "Performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with only the Age and Group columns\n",
    "age_df = Performance_df[['Age', 'Group']]\n",
    "\n",
    "# Split age_df into control and ADHD groups\n",
    "control_df = age_df[age_df['Group'] == 0]\n",
    "adhd_df = age_df[age_df['Group'] == 1]\n",
    "\n",
    "# Count the number of observations for each age in the control and ADHD groups\n",
    "control_counts = control_df['Age'].value_counts().sort_index()\n",
    "adhd_counts = adhd_df['Age'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar chart with the age on the x-axis and the count on the y-axis\n",
    "bar_width = 0.35\n",
    "plt.bar(control_counts.index, control_counts.values, bar_width, color='blue', alpha=0.5, label='non-ADHD')\n",
    "plt.bar(adhd_counts.index + bar_width, adhd_counts.values, bar_width, color='orange', alpha=0.5, label='ADHD')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Age Distribution by Group')\n",
    "plt.legend()\n",
    "\n",
    "# Save the resulting plot as a JPEG file\n",
    "plt.savefig('Age Distribution.jpeg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f596c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Performance plot\n",
    "plt.scatter(Performance_df.Performance[:28], Performance_df.Group[:28], c='orange')\n",
    "plt.scatter(Performance_df.Performance[28:], Performance_df.Group[28:], c='blue')\n",
    "plt.xlabel('Performance')\n",
    "y_values = [\"non-ADHD\", \"ADHD\"]\n",
    "y_axis = np.arange(0, 2, 1)\n",
    "plt.yticks(y_axis, y_values)\n",
    "#plt.savefig('Performance vs Group.jpeg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea26567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Performance plot\n",
    "plt.scatter(Performance_df.Performance,Performance_df.Group)\n",
    "plt.xlabel('Performance')\n",
    "y_values = [\"non-ADHD\", \"ADHD\"]\n",
    "y_axis = np.arange(0, 2, 1)\n",
    "plt.yticks(y_axis, y_values)\n",
    "plt.savefig('Performance vs Group.jpeg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x and y\n",
    "x = Performance_df[['Age','IQscore','FD','Performance']]\n",
    "y = Performance_df[['Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1242a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create plots for the WISC data\n",
    "WISCParameters = Performance_df[['IQscore','FD']]\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n",
    "fig.subplots_adjust(top=0.85, bottom=0.1, left=0.1, right=0.97, wspace=0.3, hspace=0.7)\n",
    "\n",
    "# Flatten the axes array into a 1-dimensional array\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Create a scatter plot for each column in a separate subplot\n",
    "for i in range(WISCParameters.shape[1]):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(WISCParameters.iloc[:, i], Performance_df['Group'])\n",
    "    if i == 0:\n",
    "        ax.set_xlabel('Full Scale IQ')\n",
    "    else:\n",
    "        ax.set_xlabel('Freedom from Distractibility')\n",
    "    ax.set_ylabel('Group')\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['non-ADHD', 'ADHD'])\n",
    "    ax.autoscale()\n",
    "\n",
    "# Set the title of the plot\n",
    "fig.suptitle('WISC vs Group')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "\n",
    "# Save the resulting plot as a JPEG file\n",
    "plt.savefig('WISC Distribution.jpeg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot for the Age and the IQ\n",
    "plt.scatter(Performance_df['Age'][28:50],Performance_df['IQscore'][28:50], color='blue', label='non-ADHD')\n",
    "plt.scatter(Performance_df['Age'][0:28],Performance_df['IQscore'][0:28], marker='x', color='orange', label='ADHD')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Full-Scale IQ score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('Age vs IQ_all.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined plots\n",
    "BenchmarkParameters = Performance_df[['Age','IQscore','FD','Performance']]\n",
    "\n",
    "# Set up the figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10), sharey=True)\n",
    "fig.subplots_adjust(top=0.9, bottom=0.1, left=0.1, right=0.97, wspace=0.3, hspace=0.7)\n",
    "\n",
    "# Create a scatter plot for each column in a separate subplot\n",
    "for i in range(BenchmarkParameters.shape[1]):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axes[row][col]\n",
    "    ax.scatter(BenchmarkParameters.iloc[:, i], Performance_df['Group'])\n",
    "    ax.set_xlabel(BenchmarkParameters.columns[i])\n",
    "    ax.set_ylabel('Group')\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['non-ADHD', 'ADHD'])\n",
    "    ax.autoscale()\n",
    "\n",
    "# Set the title of the plot\n",
    "fig.suptitle('Performance vs Group')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "\n",
    "# Save the resulting plot as a JPEG file\n",
    "plt.savefig('Performance vs Group - Grid.jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1219b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation test with SKF\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "LG = []\n",
    "SV = []\n",
    "RF = []\n",
    "KN = []\n",
    "DT = []\n",
    "for i in range(1,43):\n",
    "    LG.append(cross_val_score(LogisticRegression(max_iter=1000,random_state =i), x, y.values.ravel(), cv=skf,scoring ='accuracy').mean())\n",
    "    SV.append(cross_val_score(SVC(random_state =i), x, y.values.ravel(), cv=skf, scoring ='accuracy').mean())\n",
    "    RF.append(cross_val_score(RandomForestClassifier(random_state =i), x, y.values.ravel(), cv=skf,scoring ='accuracy').mean())\n",
    "    DT.append(cross_val_score(DecisionTreeClassifier(random_state =i), x, y.values.ravel(), cv=skf,scoring ='accuracy').mean())\n",
    "    KN.append(cross_val_score(KNeighborsClassifier(), x, y.values.ravel(), cv=skf,scoring ='accuracy').mean())\n",
    "print(\"Accuracy of Logistic Regression is\",\"%.3f\" % round(np.mean(LG),3))\n",
    "print(\"Accuracy of SVC is\",\"%.3f\" % round(np.mean(SV),3))\n",
    "print(\"Accuracy of RandomForestClassifier is\",\"%.3f\" % round(np.mean(RF),3))\n",
    "print(\"Accuracy of KNeighborsClassifier is\",\"%.3f\" % round(np.mean(KN),3))\n",
    "print(\"Accuracy of DecisionTreeClassifier is\",\"%.3f\" % round(np.mean(DT),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f97346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation test with LOOC\n",
    "\n",
    "loocv = LeaveOneOut()\n",
    "LG = []\n",
    "SV = []\n",
    "RF = []\n",
    "KN = []\n",
    "DT = []\n",
    "for i in range(1,43):\n",
    "    LG.append(cross_val_score(LogisticRegression(random_state =i), x, y.values.ravel(), cv=loocv, scoring ='accuracy'))\n",
    "    SV.append(cross_val_score(SVC(random_state =i), x, y.values.ravel(), cv=loocv, scoring ='accuracy'))\n",
    "    RF.append(cross_val_score(RandomForestClassifier(random_state =i), x, y.values.ravel(), cv=loocv,scoring ='accuracy'))\n",
    "    DT.append(cross_val_score(DecisionTreeClassifier(random_state =i), x, y.values.ravel(), cv=loocv, scoring ='accuracy'))\n",
    "    KN.append(cross_val_score(KNeighborsClassifier(), x, y.values.ravel(), cv=loocv, scoring ='accuracy').mean())\n",
    "print(\"Accuracy of Logistic Regression is\",\"%.3f\" % round(np.mean(LG),3))\n",
    "print(\"Accuracy of SVC is\",\"%.3f\" % round(np.mean(SV),3))\n",
    "print(\"Accuracy of RandomForestClassifier is\",\"%.3f\" % round(np.mean(RF),3))\n",
    "print(\"Accuracy of KNeighborsClassifier is\",\"%.3f\" % round(np.mean(KN),3))\n",
    "print(\"Accuracy of DecisionTreeClassifier is\",\"%.3f\" % round(np.mean(DT),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03de140",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluation of the Random State impact\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "error_rate = []\n",
    "acc = []\n",
    "for i in range(1,101):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state =i)\n",
    "    LogReg = LogisticRegression(max_iter=1000)\n",
    "    LogReg.fit(X_train,y_train.values.ravel())\n",
    "    pred_i = LogReg.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test.values.ravel()))\n",
    "    acc.append(metrics.accuracy_score(y_test.values.ravel(), pred_i))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,101),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. Random State Value')\n",
    "plt.xlabel('Random State')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,101),acc,color = 'blue',linestyle='dashed', \n",
    "         marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Accuracy vs. Random State Value')\n",
    "plt.xlabel('Random State')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('Accuracy vs. Random State Value.jpeg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "#print(\"Minimum error:\",min(error_rate),\"         at Random State =\",error_rate.index(min(error_rate))+1)\n",
    "print(\"Minimum accuracy:\",min(acc),\"      at Random State =\",acc.index(min(acc))+1)\n",
    "print(\"Maximum accuracy:\",max(acc),\"      at Random State =\",acc.index(max(acc))+1)\n",
    "print(\"Average accuracy:\",\"%.3f\" % round(sum(acc)/len(acc),3))\n",
    "#print(\"Most frequent accur.:\",most_common(acc),\"  at Random State =\",acc.index(most_common(acc))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ee303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of acc\n",
    "mean_acc = np.mean(acc)\n",
    "print(\"Mean accuracy: \", mean_acc)\n",
    "\n",
    "# Calculate standard deviation of acc\n",
    "std_acc = np.std(acc)\n",
    "print(\"Standard deviation of accuracy: \", std_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY DEFAULT METHOD for Hyperparameter tuning\n",
    "\n",
    "\n",
    "rs = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y,test_size=0.02,random_state=rs)\n",
    "warnings.filterwarnings('ignore')\n",
    "# parameter grid\n",
    "parameters = {\n",
    "    'penalty' : ['l1','l2'], \n",
    "    'C'       : np.logspace(-3,3,7),\n",
    "    'solver'  : ['newton-cg','lbfgs', 'liblinear','sag','saga']\n",
    "}\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "clf = GridSearchCV(LogReg,                    # model\n",
    "                   param_grid = parameters,   # hyperparameters\n",
    "                   scoring='accuracy',        # metric for scoring\n",
    "                   cv=LeaveOneOut())          # number of folds\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "print(\"Accuracy :\",\"%.3f\" % round(clf.best_score_,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SelectKBest method\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define X and y\n",
    "X = Performance_df[['Age', 'IQscore', 'FD', 'Performance']]\n",
    "y = Performance_df[['Group']]\n",
    "\n",
    "# Split the data\n",
    "rs = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=rs)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a logistic regression model\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create a SelectKBest object\n",
    "kbest = SelectKBest(f_classif)\n",
    "\n",
    "# Create a pipeline that combines feature selection and logistic regression\n",
    "pipe = Pipeline([('kbest', kbest), ('LogReg', LogReg)])\n",
    "\n",
    "# Define parameter grid\n",
    "parameters = {\n",
    "    'kbest__k': range(1, len(X.columns) + 1),\n",
    "    'LogReg__penalty': ['l1', 'l2'],\n",
    "    'LogReg__C': np.logspace(-3, 3, 7),\n",
    "    'LogReg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "clf = GridSearchCV(pipe, param_grid=parameters, scoring='accuracy', cv=LeaveOneOut())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy\n",
    "print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "print(\"Accuracy :\",\"%.3f\" % round(clf.best_score_, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287383f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recursive Feature Elimination (RFE) with Cross-Validation along with GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define X and y\n",
    "X = Performance_df[['Age', 'IQscore', 'FD', 'Performance']]\n",
    "y = Performance_df[['Group']]\n",
    "\n",
    "# Split the data\n",
    "rs = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=rs)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a logistic regression model\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create an RFECV object\n",
    "rfecv = RFECV(estimator=LogReg, step=1, cv=LeaveOneOut(), scoring='accuracy')\n",
    "\n",
    "# Create a pipeline that combines feature selection and logistic regression\n",
    "pipe = Pipeline([('rfecv', rfecv), ('LogReg', LogReg)])\n",
    "\n",
    "# Define parameter grid\n",
    "parameters = {\n",
    "    'LogReg__penalty': ['l1', 'l2'],\n",
    "    'LogReg__C': np.logspace(-3, 3, 7),\n",
    "    'LogReg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "clf = GridSearchCV(pipe, param_grid=parameters, scoring='accuracy', cv=LeaveOneOut())\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy\n",
    "print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "print(\"Accuracy :\",\"%.3f\" % round(clf.best_score_, 3))\n",
    "\n",
    "# Retrieve the RFECV object from the best estimator\n",
    "best_rfecv = clf.best_estimator_.named_steps['rfecv']\n",
    "\n",
    "# Print the optimal number of features and the selected features\n",
    "print(\"Optimal number of features : %d\" % best_rfecv.n_features_)\n",
    "selected_features = [col for col, mask in zip(X.columns, best_rfecv.support_) if mask]\n",
    "print(\"Selected features: \", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recursive Feature Elimination (RFE) with Cross-Validation along with GridSearchCV but \n",
    "# using the Leave-One-Out cross-validation method without splitting the data into training and testing sets. \n",
    "# In this case, GridSearchCV will use the entire dataset for cross-validation and evaluate the model performance \n",
    "# by leaving one sample out in each iteration. \n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define x and y\n",
    "x = Performance_df[['Age', 'IQscore', 'FD', 'Performance']]\n",
    "y = Performance_df[['Group']]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a logistic regression model\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create an RFECV object\n",
    "rfecv = RFECV(estimator=LogReg, step=1, cv=LeaveOneOut(), scoring='accuracy')\n",
    "\n",
    "# Create a pipeline that combines feature selection and logistic regression\n",
    "pipe = Pipeline([('rfecv', rfecv), ('LogReg', LogReg)])\n",
    "\n",
    "# Define parameter grid\n",
    "parameters = {\n",
    "    'LogReg__penalty': ['l1', 'l2'],\n",
    "    'LogReg__C': np.logspace(-3, 3, 7),\n",
    "    'LogReg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Perform grid search with Leave-One-Out cross-validation\n",
    "clf = GridSearchCV(pipe, param_grid=parameters, scoring='accuracy', cv=LeaveOneOut())\n",
    "clf.fit(x, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding accuracy\n",
    "print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "print(\"Accuracy :\",\"%.3f\" % round(clf.best_score_, 3))\n",
    "\n",
    "# Retrieve the RFECV object from the best estimator\n",
    "best_rfecv = clf.best_estimator_.named_steps['rfecv']\n",
    "\n",
    "# Print the optimal number of features and the selected features\n",
    "print(\"Optimal number of features : %d\" % best_rfecv.n_features_)\n",
    "selected_features = [col for col, mask in zip(x.columns, best_rfecv.support_) if mask]\n",
    "print(\"Selected features: \", selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d30054",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x)\n",
    "Y = np.array(y)\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "cm=np.zeros(shape=(2,2))\n",
    "for train_index, test_index in loo.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    #LogReg = LogisticRegression(C=10, penalty= 'l2', solver= 'newton-cg')\n",
    "    LogReg = LogisticRegression(C=10, penalty= 'l1', solver= 'liblinear')\n",
    "    LogReg.fit(X_train,np.array(y_train))\n",
    "    y_pred = LogReg.predict(np.array(X_test) )\n",
    "    value_y_test = y_test[0][0] \n",
    "    value_y_pred = int(y_pred[0])\n",
    "    cm[value_y_test][value_y_pred] +=1\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm)\n",
    "cm_display.plot()\n",
    "\n",
    "plt.savefig('confusion_matrix.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix performance metrics\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of the moving average accuracy\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "error_rate = []\n",
    "acc = []\n",
    "average_accuracy = []\n",
    "for i in range(1,5001):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.02,random_state =i)\n",
    "    LogReg = LogisticRegression(max_iter=1000,C=10, penalty= 'l1', solver= 'liblinear')\n",
    "    LogReg.fit(X_train,y_train.values.ravel())\n",
    "    pred_i = LogReg.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test.values.ravel()))\n",
    "    acc.append(metrics.accuracy_score(y_test.values.ravel(), pred_i))\n",
    "    average_accuracy.append(round(sum(acc)/len(acc),3))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,5001),average_accuracy,color='blue')\n",
    "plt.title('Moving Average Accuracy')\n",
    "plt.xlabel('Random State')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlim(2, 5001)\n",
    "plt.ylim(0.6, 1)\n",
    "plt.savefig('Moving Average Accuracy.jpeg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ceb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT\n",
    "# since there should be no distinction between the iq of the ADHD group and the ctrl group, we can use mean of ctrl\n",
    "iqmean = x.IQscore[28:50].mean()\n",
    "agemean = x.Age[28:50].mean()\n",
    "fdmean = x.FD[28:50].mean()\n",
    "iqmax = x.IQscore.max()\n",
    "agemax = x.Age.max()\n",
    "fdmax = x.FD.max()\n",
    "iqmin = x.IQscore.min()\n",
    "agemin = x.Age.min()\n",
    "fdmin = x.FD.min()\n",
    "n=101\n",
    "j=0\n",
    "zx=[0]*n\n",
    "zy=[0]*n\n",
    "for i in np.linspace(-6, 6, n):\n",
    "    a=x.Performance.min()\n",
    "    b=x.Performance.max()\n",
    "    zx[j] = a+(((i-(-6))*(b-a))/(6-(-6)))\n",
    "    zy[j] = expit(LogReg.intercept_[0]+LogReg.coef_[0][0]*agemean+LogReg.coef_[0][1]*iqmean+LogReg.coef_[0][2]*fdmean+LogReg.coef_[0][3]*i)\n",
    "    j=j+1\n",
    "#print(zx)\n",
    "plt.plot(zx,zy,color='black',linewidth=0.5);\n",
    "plt.xlabel('Performance')\n",
    "y_values = [\"Control\", \"ADHD\"]\n",
    "y_axis = np.arange(0, 2, 1)\n",
    "plt.yticks(y_axis, y_values)\n",
    "plt.xlim(x.Performance.min(), x.Performance.max())\n",
    "#plt.legend(loc=\"center left\")\n",
    "plt.title('Logistic Regression curve for mean Age, IQ and FD')\n",
    "plt.savefig('LogRegCurve.jpeg', dpi=300, bbox_inches='tight')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
